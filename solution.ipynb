{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a81b691-476d-4bfa-bb30-d861458912f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from ipywidgets import interact, Dropdown\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import trec\n",
    "import pprint\n",
    "import json\n",
    "import copy\n",
    "import pickle\n",
    "import re\n",
    "from bertviz import model_view, head_view\n",
    "from transformers import *\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "\n",
    "\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "    \n",
    "pp = pprint.PrettyPrinter(width=120, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976351a4-92c4-4c95-9f07-e7290304ea73",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Patient Case Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7650353-0d54-4858-be34-252bebb6ea57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Queries = \"topics-2014_2015-summary.topics\"\n",
    "Qrels = \"qrels-clinical_trials.txt\"\n",
    "with open(Queries, 'r') as queries_reader:\n",
    "    txt = queries_reader.read()\n",
    "\n",
    "root = ET.fromstring(txt)\n",
    "cases = {}\n",
    "for query in root.iter('TOP'):\n",
    "    q_num = query.find('NUM').text\n",
    "    q_title = query.find('TITLE').text\n",
    "    cases[q_num] = q_title\n",
    "\n",
    "eval = trec.TrecEvaluation(cases, Qrels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf1b380-f7ad-49bb-bb83-14799a828239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pp.pprint(cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6278960c",
   "metadata": {},
   "source": [
    "# Loading stop words file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63c709a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gist_file = open(\"stopWords.txt\", \"r\")\n",
    "try:\n",
    "    content = gist_file.read()\n",
    "    stop_words = content.split(\",\")\n",
    "finally:\n",
    "    gist_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0c6eb0-9fbf-434f-bd84-e8c7c5260786",
   "metadata": {},
   "source": [
    "# Define Clinical Trial Document Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc0dc88-9af9-427b-8fb6-52a43c1d1862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trial:\n",
    "    _nct_id : str\n",
    "    _brief_title : str\n",
    "    _detailed_description : str\n",
    "    _brief_summary : str\n",
    "    _criteria : str\n",
    "    _phase : str\n",
    "    _study_type : str\n",
    "    _study_design : str\n",
    "    _condition : str\n",
    "    _intervention : {}\n",
    "    _gender : str\n",
    "    _min_age : int\n",
    "    _max_age : int\n",
    "    _healthy_volunteers : str\n",
    "    _mesh_terms : []\n",
    "\n",
    "    def __init__(self):\n",
    "        self._nct_id = \"\"\n",
    "        self._intervention = {}\n",
    "        self._mesh_terms = []\n",
    "\n",
    "    def show(self):\n",
    "        print(json.dumps(self.__dict__, indent=4))\n",
    "\n",
    "def cleanstr(txt):\n",
    "    return re.sub(' +', ' ', txt.strip().replace('\\n',''))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d2efe0-1f95-423d-8d8b-3c70ebc9cee5",
   "metadata": {},
   "source": [
    "# Load the clinical trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da63f68",
   "metadata": {},
   "source": [
    "Load from bin files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddf8f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ids and documents\n",
    "ids = pickle.load( open( \"doc_ids.bin\", \"rb\" ) )\n",
    "full_docs = pickle.load( open( \"full_documents.bin\", \"rb\" ) )\n",
    "doc_dict = dict(zip(ids, full_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8aa36e",
   "metadata": {},
   "source": [
    "Load from tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18df5689-15fd-405c-9e5f-24109e1422fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tar = tarfile.open(\"clinicaltrials.gov-16_dec_2015.tgz\", \"r:gz\")\n",
    "i = 0\n",
    "ids = []\n",
    "full_docs = []\n",
    "for tarinfo in tar:\n",
    "    if tarinfo.size > 500:\n",
    "        txt = tar.extractfile(tarinfo).read().decode(\"utf-8\", \"strict\")\n",
    "        root = ET.fromstring(txt)\n",
    "\n",
    "        judged = False\n",
    "        for doc_id in root.iter('nct_id'):\n",
    "            if doc_id.text in eval.judged_docs:\n",
    "                judged = True\n",
    "\n",
    "        if judged is False:\n",
    "            continue\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "        trial = Trial()\n",
    "        \n",
    "        for brief_title in root.iter('brief_title'):\n",
    "            ids.append(doc_id.text)\n",
    "            trial._nct_id = cleanstr(doc_id.text)\n",
    "            trial._brief_title = cleanstr(brief_title.text)\n",
    "\n",
    "        trial._detailed_description = trial._brief_title\n",
    "        for detailed_description in root.iter('detailed_description'):\n",
    "            for child in detailed_description:\n",
    "                trial._detailed_description = cleanstr(child.text)\n",
    "\n",
    "        trial._brief_summary = trial._brief_title\n",
    "        for brief_summary in root.iter('brief_summary'):\n",
    "            for child in brief_summary:\n",
    "                trial._brief_summary = cleanstr(child.text)\n",
    "\n",
    "        trial._criteria = trial._brief_title\n",
    "        for criteria in root.iter('criteria'):\n",
    "            for child in criteria:\n",
    "                trial._criteria = cleanstr(child.text)\n",
    "                \n",
    "        trial._phase = trial._brief_title\n",
    "        for phase in root.iter('phase'):\n",
    "            trial._phase = cleanstr(phase.text)\n",
    "\n",
    "        for study_type in root.iter('study_type'):\n",
    "            trial._study_type = cleanstr(study_type.text)\n",
    "            \n",
    "        for study_design in root.iter('study_design'):\n",
    "            trial._study_design = cleanstr(study_design.text)\n",
    "            \n",
    "        trial._condition = trial._brief_title\n",
    "        for condition in root.iter('condition'):\n",
    "            trial._condition = cleanstr(condition.text)\n",
    "\n",
    "        for interventions in root.iter('intervention'):\n",
    "            for child in interventions:\n",
    "                trial._intervention[cleanstr(child.tag)] = cleanstr(child.text)\n",
    "\n",
    "        trial._gender = \"both\"\n",
    "        for gender in root.iter('gender'):\n",
    "            trial._gender = cleanstr(gender.text)\n",
    "            \n",
    "        trial._minimum_age = 0\n",
    "        for minimum_age in root.iter('minimum_age'):\n",
    "            age = re.findall('[0-9]+', cleanstr(minimum_age.text))\n",
    "            if age:\n",
    "                trial._minimum_age = int(age[0])\n",
    "            else:\n",
    "                trial._minimum_age = 0\n",
    "            \n",
    "        trial._maximum_age = 150\n",
    "        for maximum_age in root.iter('maximum_age'):\n",
    "            age = re.findall('[0-9]+', cleanstr(maximum_age.text))\n",
    "            if age:\n",
    "                trial._maximum_age = int(age[0])\n",
    "            else:\n",
    "                trial._maximum_age = 150\n",
    "               \n",
    "            \n",
    "        trial._healthy_volunteers = trial._brief_title\n",
    "        for healthy_volunteers in root.iter('healthy_volunteers'):\n",
    "            trial._healthy_volunteers = cleanstr(healthy_volunteers.text)\n",
    "            \n",
    "        for mesh_term in root.iter('mesh_term'):\n",
    "            trial._mesh_terms.append(cleanstr(mesh_term.text))\n",
    "        \n",
    "        full_docs.append(trial)\n",
    "        \n",
    "tar.close()\n",
    "\n",
    "print(\"Total of clinical trials: \", i)\n",
    "\n",
    "pickle.dump(ids, open( \"doc_ids.bin\", \"wb\" ) )\n",
    "pickle.dump(full_docs, open( \"full_documents.bin\", \"wb\" ) )\n",
    "doc_dict = dict(zip(ids, full_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f49b0e-db82-4b65-a25f-8c8d1d23995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a document\n",
    "pp.pprint(vars(full_docs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41414d0-bc3c-4c74-9ca0-83affc1a18df",
   "metadata": {},
   "source": [
    "# Retrieval Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3020e537-84cb-4cdd-bb66-89ec6b1eb249",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class VSMindex:\n",
    "    def __init__(self, corpus, _ngram_range=(1, 1), _analyzer='word', _stop_words=None):\n",
    "        self.vectorizer = TfidfVectorizer(ngram_range=_ngram_range, analyzer=_analyzer, stop_words=_stop_words)\n",
    "        self.count_matrix = self.vectorizer.fit_transform(corpus)\n",
    "\n",
    "    def search(self, query):\n",
    "        query_vector = self.vectorizer.transform([query])\n",
    "        doc_scores = (1 - pairwise_distances(query_vector, self.count_matrix, metric='cosine')).flatten()\n",
    "    \n",
    "        return doc_scores\n",
    "\n",
    "\n",
    "class LMJMindex:\n",
    "    \n",
    "    def __init__(self, corpus, _ngram_range=(1,1), _analyzer='word', _stop_words=None):\n",
    "        self.vectorizer = CountVectorizer(ngram_range=_ngram_range, analyzer=_analyzer, stop_words=_stop_words)\n",
    "        self.count_matrix = self.vectorizer.fit_transform(corpus)\n",
    "        \n",
    "        term_freq = np.sum(self.count_matrix, axis=0)\n",
    "\n",
    "        doc_len = np.sum(self.count_matrix, axis=1) \n",
    "\n",
    "        self.prob_term_col = term_freq / np.sum(term_freq)\n",
    "\n",
    "        self.prob_term_doc = self.count_matrix / doc_len\n",
    "        \n",
    "        # Set initial lambda value     \n",
    "        params = {'lambda' : 0.3}\n",
    "        self.set_params(params)\n",
    "\n",
    "        \n",
    "    def set_params(self, params):\n",
    "        if 'lambda' in params:\n",
    "            self.lbd = params['lambda']\n",
    "            self._log_lmjm = np.log(self.lbd * self.prob_term_doc + (1 - self.lbd) * self.prob_term_col)\n",
    "            print(\"LMJM lambda \", self.lbd)\n",
    "\n",
    "            \n",
    "    def search(self, query):\n",
    "        query_vector = self.vectorizer.transform([query])\n",
    "\n",
    "        #prob_term_query = query_vector / np.sum(query_vector)\n",
    "\n",
    "        # np.array is used with flatten so instead of the operation resulting in a nx1 matrix it results in an array\n",
    "        doc_scores = np.array(np.sum(query_vector.multiply(self._log_lmjm), axis=1)).flatten()\n",
    "\n",
    "        return doc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f470c4-4ebd-4490-9559-1458bd470d6c",
   "metadata": {},
   "source": [
    "# LETOR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b8e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'dmis-lab/biobert-v1.1'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "config = AutoConfig.from_pretrained(model_path,  output_hidden_states=True, output_attentions=True)  \n",
    "model = AutoModel.from_pretrained(model_path, config=config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e849ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cls(query_pairs, embeddings, batch_size=32):\n",
    "\n",
    "    # Iterate over all documents, in batches of size <batch_size>\n",
    "    for batch_idx in range(0, len(query_pairs), batch_size):\n",
    "        # Print how many batches have been processed and how many are left\n",
    "        #print(f'Processing batch {1 + batch_idx/batch_size} of {round(len(query_pairs)/batch_size)}')\n",
    "\n",
    "        # Get the current batch of samples\n",
    "        batch_data = query_pairs[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "        inputs = tokenizer.batch_encode_plus(batch_data, \n",
    "                                       return_tensors='pt',  # pytorch tensors\n",
    "                                       add_special_tokens=True,  # Add CLS and SEP tokens\n",
    "                                       max_length = 512, # Max sequence length\n",
    "                                       truncation = True, # Truncate if sequences exceed the Max Sequence length\n",
    "                                       padding = True) # Add padding to forward sequences with different lengths\n",
    "        \n",
    "        # Forward the batch of (query, doc) sequences\n",
    "        with torch.no_grad():\n",
    "            inputs.to(device)\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Get the CLS embeddings for each pair query, document\n",
    "        batch_cls = outputs['hidden_states'][-1][:,0,:]\n",
    "        \n",
    "        # L2-Normalize CLS embeddings. Embeddings norm will be 1.\n",
    "        batch_cls = torch.nn.functional.normalize(batch_cls, p=2, dim=1)\n",
    "        \n",
    "        # Store the extracted CLS embeddings from the batch on the memory-mapped ndarray\n",
    "        embeddings[batch_idx:batch_idx + batch_size] = batch_cls.cpu()\n",
    "        \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27120212-d19d-49c2-8aff-826a9809c50f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class LETORindex:\n",
    "    \n",
    "    def __init__(self, model, dataset):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "\n",
    "        #self.scaler = StandardScaler()\n",
    "        \n",
    "        #test_queries = [\"20155\", \"201514\", \"20152\", \"201512\", \"201524\", \"20154\", \"201423\", \"201429\", \"201413\", \"20144\"]\n",
    "        \"\"\"\n",
    "        training_dataset, test_dataset = self.split_dataset_on_queries(self.dataset, test_queries)\n",
    "            \n",
    "        X_train, y_train = self.get_X_scaled_and_y(training_dataset)\n",
    "        self.X_test, self.y_test = self.get_X_scaled_and_y(test_dataset)\n",
    "            \n",
    "        self.y_test = self.y_test.replace(2,1)\n",
    "        \n",
    "        \n",
    "        y_train = y_train.replace(2,1)\n",
    "        self.model.fit(X_train, y_train)            \n",
    "        \"\"\"\n",
    "        X = self.dataset.drop(columns=['case_id', 'doc_id', 'y'])\n",
    "        y = self.dataset['y']\n",
    "        \n",
    "\n",
    "        w2 = len(y[y==0]) / len(y[y==2])\n",
    "        w1 = len(y[y==0]) / len(y[y==1])\n",
    "        w0 = 1\n",
    "\n",
    "        weights = {0.0: w0, 1.0: w1, 2.0: w2}\n",
    "\n",
    "        sample_weight = y.map(weights)\n",
    "        y = y.replace(2.0,1.0)\n",
    "\n",
    "        #X_scaled = self.scaler.fit_transform(X)\n",
    "\n",
    "        self.model.fit(X, y, sample_weight=sample_weight)\n",
    "\n",
    "        \n",
    "    def split_dataset_on_queries(self, dataset, queries):\n",
    "        # Training dataset, filters rows that do not have test_queries\n",
    "        train_dataset = dataset[~dataset['case_id'].isin(queries)]\n",
    "\n",
    "        train_dataset.to_csv(\"csv/train.csv\", index=False)\n",
    "\n",
    "        # Test dataset, filters rows with test_queries\n",
    "        test_dataset = dataset[dataset['case_id'].isin(queries)]\n",
    "\n",
    "        test_dataset.to_csv(\"csv/test.csv\", index=False)\n",
    "        \n",
    "        return train_dataset, test_dataset\n",
    "    \n",
    "\n",
    "            \n",
    "    def search(self, query):\n",
    "        # Reverse the dictionary: create a new dictionary mapping values to keys\n",
    "        reverse_dict = {v: k for k, v in cases.items()}\n",
    "\n",
    "        # Get query id from query\n",
    "        query_id = reverse_dict[query]\n",
    "\n",
    "        query_embeddings = self.dataset[self.dataset['case_id'] == query_id].drop(columns=['case_id', 'doc_id', 'y'])\n",
    "        \n",
    "\n",
    "        #scaled_query_embeddings = self.scaler.transform(query_embeddings)\n",
    "\n",
    "        doc_scores = query_embeddings.dot(self.model.coef_[0].T)\n",
    "        \n",
    "        return doc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acecb4b",
   "metadata": {},
   "source": [
    "Load dataset from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4737ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe if it already exists\n",
    "dataset = pd.read_csv('csv/dataset.csv')\n",
    "# Change type of case_id to string\n",
    "dataset['case_id'] = dataset['case_id'].astype(str)\n",
    "\n",
    "# Load embeddings if they already exist\n",
    "#embeddings_df = pd.read_csv('csv/embeddings.csv')\n",
    "# Change type of case_id to string\n",
    "#embeddings_df['case_id'] = embeddings_df['case_id'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663aa977",
   "metadata": {},
   "source": [
    "Create dataset if it does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ecf643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "query_document_pairs_df = pd.DataFrame(product(cases.keys(), ids), columns=['case_id', 'doc_id'])\n",
    "\n",
    "query_document_pairs = []\n",
    "for case_id in cases:\n",
    "    query = cases[case_id]\n",
    "    for doc_id in doc_dict:\n",
    "        doc = doc_dict[doc_id]._detailed_description\n",
    "        query_document_pairs.append((query, doc))\n",
    "        \n",
    "\n",
    "# Create dataset from text file with y values (relevant or non relevant) for each query/document pair\n",
    "relevancy_df = pd.read_csv('qrels-clinical_trials.txt', \n",
    "                           delim_whitespace=True, \n",
    "                           names=['case_id', 'x', 'doc_id','y'], \n",
    "                           dtype={'case_id': str}).drop('x', axis=1)\n",
    "\n",
    "relevancy_df.to_csv(\"csv/relevancy.csv\", index=False)\n",
    "\n",
    "# Merge dataset with text file for y values (relevant or non relevant) for each query/document pair\n",
    "dataset_without_embeddings = query_document_pairs_df.merge(relevancy_df, on=['case_id', 'doc_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac410f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING, EXTREMELY LONG EXECUTION TIME\n",
    "# Numpy ndarray that will store (in RAM) the CLS embeddings of each (query, doc) pair\n",
    "embeddings = np.zeros((len(query_document_pairs), 768))\n",
    "\n",
    "# Extract the embedding of the CLS token of the last layer for each (query, doc) pair\n",
    "embeddings = extract_cls(query_document_pairs, embeddings=embeddings, batch_size=8)\n",
    "\n",
    "embeddings_df = pd.DataFrame(embeddings)\n",
    "embeddings_df = pd.concat([query_document_pairs_df, embeddings_df], axis=1)\n",
    "embeddings_df.to_csv(\"csv/embeddings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eab69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataset with the CLS embeddings and the y values\n",
    "dataset = dataset_without_embeddings.merge(embeddings_df, on=['case_id', 'doc_id'], how='left').fillna(0)\n",
    "dataset.to_csv(\"csv/dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516e7aaf-a31d-4d35-be2a-b958d27dae10",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ee9f8e",
   "metadata": {},
   "source": [
    "VSM and LMJM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fece409-b0f6-4803-8693-bd855f98cd47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "ids = pickle.load( open( \"doc_ids.bin\", \"rb\" ) )\n",
    "full_docs = pickle.load( open( \"full_documents.bin\", \"rb\" ) )\n",
    "\n",
    "corpus_brief_title = []\n",
    "corpus_brief_summary = []\n",
    "corpus_detailed_description = []\n",
    "corpus_criteria = []\n",
    "corpus_full = []\n",
    "for trial in full_docs:\n",
    "    corpus_brief_title.append(trial._brief_title)\n",
    "    corpus_brief_summary.append(trial._brief_summary)\n",
    "    corpus_detailed_description.append(trial._detailed_description)\n",
    "    corpus_criteria.append(trial._criteria)\n",
    "\n",
    "    full = trial._brief_title + trial._brief_summary + trial._detailed_description + trial._criteria\n",
    "    corpus_full.append(full)\n",
    "\n",
    "indexes_list = {}\n",
    "\n",
    "indexes_list['vsm_brief_title'] = VSMindex(corpus_brief_title, _ngram_range=(1,1), _analyzer='word', _stop_words = stop_words)\n",
    "indexes_list['vsm_brief_summary'] = VSMindex(corpus_brief_summary, _ngram_range=(1,1), _analyzer='word', _stop_words = stop_words)\n",
    "indexes_list['vsm_detailed_description'] = VSMindex(corpus_detailed_description, _ngram_range=(1,1), _analyzer='word', _stop_words = stop_words)\n",
    "indexes_list['vsm_criteria'] = VSMindex(corpus_criteria, _ngram_range=(1,1), _analyzer='word', _stop_words = stop_words)\n",
    "indexes_list['vsm_full'] = VSMindex(corpus_full, _ngram_range=(1,1), _analyzer='word', _stop_words = stop_words)\n",
    "\n",
    "indexes_list['lmjm_brief_title'] = LMJMindex(corpus_brief_title, _ngram_range=(1,1), _analyzer='word', _stop_words = stop_words)\n",
    "indexes_list['lmjm_brief_summary'] = LMJMindex(corpus_brief_summary, _ngram_range=(1,1), _analyzer='word', _stop_words = stop_words)\n",
    "indexes_list['lmjm_detailed_description'] = LMJMindex(corpus_detailed_description, _ngram_range=(1,1), _analyzer='word', _stop_words = stop_words)\n",
    "indexes_list['lmjm_criteria'] = LMJMindex(corpus_criteria, _ngram_range=(1,1), _analyzer='word', _stop_words = stop_words)\n",
    "indexes_list['lmjm_full'] = LMJMindex(corpus_full, _ngram_range=(1,1), _analyzer='word', _stop_words = stop_words)\n",
    "\n",
    "#pickle.dump(indexes_list, open( \"indexes_list.bin\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0771208d",
   "metadata": {},
   "source": [
    "LETOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa42e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "C = 0.5\n",
    "\n",
    "indexes_list['LETOR_model'] = LETORindex(LogisticRegression(C=C, max_iter=1000), dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34a99dc-d989-4745-93bb-f31062d82826",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922c1356-026f-440d-a855-322bb714658d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index_name in indexes_list:\n",
    "    print()\n",
    "    print(index_name)\n",
    "    index = indexes_list[index_name]\n",
    "    index.doc_scores = {}\n",
    "    index.results_ord = {}\n",
    "    for caseid in cases:\n",
    "        query = cases[caseid]\n",
    "\n",
    "        doc_scores = index.search(query)\n",
    "\n",
    "        index.doc_scores[caseid] = doc_scores\n",
    "        results = pd.DataFrame(list(zip(ids, doc_scores)), columns = ['_id', 'score'])\n",
    "        index.results_ord[caseid] = results.sort_values(by=['score'], ascending = False)\n",
    "\n",
    "    indexes_list[index_name] = index\n",
    "\n",
    "#pickle.dump(indexes_list, open( \"indexes_list.bin\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89fbeb1",
   "metadata": {},
   "source": [
    "LETOR only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880b9e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_name in indexes_list:\n",
    "    if index_name != 'LETOR_model':\n",
    "        continue\n",
    "    print()\n",
    "    print(index_name)\n",
    "    index = indexes_list[index_name]\n",
    "    index.doc_scores = {}\n",
    "    index.results_ord = {}\n",
    "    i = 1\n",
    "    for caseid in cases:\n",
    "        query = cases[caseid]\n",
    "\n",
    "        doc_scores = index.search(query)\n",
    "\n",
    "        index.doc_scores[caseid] = doc_scores\n",
    "        results = pd.DataFrame(list(zip(ids, doc_scores)), columns = ['_id', 'score'])\n",
    "        index.results_ord[caseid] = results.sort_values(by=['score'], ascending = False)\n",
    "        \n",
    "        i = i + 1\n",
    "\n",
    "    indexes_list[index_name] = index\n",
    "\n",
    "#pickle.dump(indexes_list, open( \"indexes_list.bin\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac98603-aa43-4ef0-950d-fa0d1306a9a3",
   "metadata": {},
   "source": [
    "## Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2299403a-1b83-4a35-a0b4-1a24b1b86d78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def check_words_in_query(query, target_words):\n",
    "    for target_word in target_words:\n",
    "        pattern = re.compile(r'\\b{}\\b|\\b{}-\\b'.format(target_word, target_word), re.IGNORECASE)\n",
    "\n",
    "        match = re.search(pattern, query)\n",
    "        \n",
    "        if bool(match):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def extract_age_and_gender(query):\n",
    "    age_pattern = re.compile(r'\\b(\\d{1,3})\\b(?: ?(?:-|years?-?)? ?(?:old|yo))?\\b')\n",
    "    gender_patterns = re.compile(r'\\b(?:man|boy|male|woman|women|girl|female)\\b', re.IGNORECASE)\n",
    "\n",
    "    # Extract age\n",
    "    default_age = 150\n",
    "    age_match = age_pattern.search(query)\n",
    "    # Check if the query contains relevant terms indicating age\n",
    "    age_terms_present = check_words_in_query(query ,[\"year\", \"month\", \"yo\"])\n",
    "    \n",
    "    if age_match and age_terms_present:        \n",
    "        age_value = int(age_match.group(1))\n",
    "        age = age_value / 12 if 'month' in query.lower() and 'year' not in query.lower() else age_value\n",
    "        age = age if age_terms_present else default_age\n",
    "        \n",
    "    else:\n",
    "        young_adult = check_words_in_query(query, [\"young\"])\n",
    "        default_age = 21 if young_adult else default_age\n",
    "        age = default_age\n",
    "\n",
    "    # Extract gender\n",
    "    default_gender = 'Unknown'\n",
    "    gender_match = gender_patterns.search(query)\n",
    "    gender = gender_match.group(0).lower() if gender_match else default_gender\n",
    "\n",
    "    # Standardize gender\n",
    "    if gender in ['man', 'boy', 'male']:\n",
    "        gender = 'Male'\n",
    "    elif gender in ['woman', 'women', 'girl', 'female']:\n",
    "        gender = 'Female'    \n",
    "    \n",
    "\n",
    "    return age, gender\n",
    "\n",
    "\n",
    "\n",
    "def filter_by_age_and_gender(doc_scores, query):\n",
    "    \n",
    "    age, gender = extract_age_and_gender(query)\n",
    "    \n",
    "    doc_scores = list(doc_scores)\n",
    "    \n",
    "    for i in range(len(full_docs)):\n",
    "        trial = vars(full_docs[i])\n",
    "        \n",
    "        age_check = trial['_minimum_age'] <= age and age <= trial['_maximum_age']\n",
    "        \n",
    "        \n",
    "        gender_check = trial['_gender'] == 'Both' or trial['_gender'] == gender\n",
    "\n",
    "        \n",
    "        # Filter unwanted documents\n",
    "        # Filter document i (CHECK IF IT IS WORKING)\n",
    "        if not (age_check and gender_check):\n",
    "            doc_scores[i] = -1000000\n",
    "    \n",
    "\n",
    "    #return filtered_ids\n",
    "    return doc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f2545f-227d-4350-b4f9-761ad042d24e",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "# Compute evaluation metric results\n",
    "For each patient, search each index and rank clinical trials by their similarity to the patient case description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef5c868-ba30-46d0-9f2f-775e87c9105a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index_name in indexes_list:\n",
    "    index = indexes_list[index_name]\n",
    "    print()\n",
    "    print(index_name)\n",
    "\n",
    "    m_ap = 0\n",
    "    m_p10 = 0\n",
    "    m_mrr = 0\n",
    "    m_ndcg5 = 0\n",
    "    m_recall = 0\n",
    "    mean_precision_11point = np.zeros(11)\n",
    "    \n",
    "    index.p10_per_query = {}\n",
    "    index.ndcg5_per_query = {}\n",
    "    index.recall_per_query = {}\n",
    "    index.mrr_per_query = {}\n",
    "    index.ap_per_query = {}\n",
    "    index.precision_11point_per_query = {}\n",
    "\n",
    "    for caseid in cases:\n",
    "        query = cases[caseid]\n",
    "\n",
    "        results_ord = index.results_ord[caseid]\n",
    "        #p10 = eval.fast_p10(results_ord, caseid)\n",
    "\n",
    "        doc_scores = copy.deepcopy(index.doc_scores[caseid])\n",
    "        doc_scores = filter_by_age_and_gender(doc_scores, query)\n",
    "        results = pd.DataFrame(list(zip(ids, doc_scores)), columns = ['_id', 'score'])\n",
    "        results_ord = results.sort_values(by=['score'], ascending = False)\n",
    "        #p10 = eval.fast_p10(results_ord, caseid)\n",
    "\n",
    "        [p10, recall, ap, ndcg5, mrr] = eval.eval(results_ord, caseid)\n",
    "        [precision_11point, recall_11point, total_relv_ret] = eval.evalPR(results_ord, caseid)\n",
    "\n",
    "        if (np.shape(recall_11point) != (0,)):\n",
    "            mean_precision_11point = mean_precision_11point + precision_11point\n",
    "        \n",
    "        index.p10_per_query[caseid] = p10\n",
    "        index.ndcg5_per_query[caseid] = ndcg5\n",
    "        index.recall_per_query[caseid] = recall\n",
    "        index.mrr_per_query[caseid] = mrr\n",
    "        index.ap_per_query[caseid] = ap\n",
    "        index.precision_11point_per_query[caseid] = precision_11point\n",
    "\n",
    "        m_ap = m_ap + ap\n",
    "        m_p10 = m_p10 + p10\n",
    "        m_mrr = m_mrr + mrr\n",
    "        m_ndcg5 = m_ndcg5 + ndcg5\n",
    "        m_recall = m_recall + recall\n",
    "\n",
    "    index.m_ap = m_ap / len(cases)\n",
    "    index.m_p10 = m_p10 / len(cases)\n",
    "    index.m_mrr = m_mrr / len(cases)\n",
    "    index.m_ndcg5 = m_ndcg5 / len(cases)\n",
    "    index.m_recall = m_recall / len(cases)\n",
    "    index.mean_precision_11point = mean_precision_11point/len(cases)\n",
    "\n",
    "    print(\"   P10    \", index.m_p10)\n",
    "    print(\"   NDCG@5 \", index.m_ndcg5)\n",
    "    print(\"   MRR    \", index.m_mrr)\n",
    "    print(\"   MAP    \", index.m_ap)\n",
    "    print(\"   Recall \", index.m_recall)\n",
    "\n",
    "    indexes_list[index_name] = index\n",
    "\n",
    "pickle.dump(indexes_list, open( \"indexes_results.bin\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d730a478",
   "metadata": {},
   "source": [
    "LETOR only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f935b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_name in indexes_list:\n",
    "    if index_name != 'LETOR_model':\n",
    "        continue\n",
    "    index = indexes_list[index_name]\n",
    "    print()\n",
    "    print(index_name)\n",
    "\n",
    "    m_ap = 0\n",
    "    m_p10 = 0\n",
    "    m_mrr = 0\n",
    "    m_ndcg5 = 0\n",
    "    m_recall = 0\n",
    "    mean_precision_11point = np.zeros(11)\n",
    "    \n",
    "    index.p10_per_query = {}\n",
    "    index.ndcg5_per_query = {}\n",
    "    index.recall_per_query = {}\n",
    "    index.mrr_per_query = {}\n",
    "    index.ap_per_query = {}\n",
    "    index.precision_11point_per_query = {}\n",
    "\n",
    "    for caseid in cases:\n",
    "        query = cases[caseid]\n",
    "\n",
    "        results_ord = index.results_ord[caseid]\n",
    "        #p10 = eval.fast_p10(results_ord, caseid)\n",
    "\n",
    "        doc_scores = copy.deepcopy(index.doc_scores[caseid])\n",
    "        doc_scores = filter_by_age_and_gender(doc_scores, query)\n",
    "        results = pd.DataFrame(list(zip(ids, doc_scores)), columns = ['_id', 'score'])\n",
    "        results_ord = results.sort_values(by=['score'], ascending = False)\n",
    "        #p10 = eval.fast_p10(results_ord, caseid)\n",
    "\n",
    "        [p10, recall, ap, ndcg5, mrr] = eval.eval(results_ord, caseid)\n",
    "        [precision_11point, recall_11point, total_relv_ret] = eval.evalPR(results_ord, caseid)\n",
    "\n",
    "        if (np.shape(recall_11point) != (0,)):\n",
    "            mean_precision_11point = mean_precision_11point + precision_11point\n",
    "        \n",
    "        index.p10_per_query[caseid] = p10\n",
    "        index.ndcg5_per_query[caseid] = ndcg5\n",
    "        index.recall_per_query[caseid] = recall\n",
    "        index.mrr_per_query[caseid] = mrr\n",
    "        index.ap_per_query[caseid] = ap\n",
    "        index.precision_11point_per_query[caseid] = precision_11point\n",
    "\n",
    "        m_ap = m_ap + ap\n",
    "        m_p10 = m_p10 + p10\n",
    "        m_mrr = m_mrr + mrr\n",
    "        m_ndcg5 = m_ndcg5 + ndcg5\n",
    "        m_recall = m_recall + recall\n",
    "\n",
    "    index.m_ap = m_ap / len(cases)\n",
    "    index.m_p10 = m_p10 / len(cases)\n",
    "    index.m_mrr = m_mrr / len(cases)\n",
    "    index.m_ndcg5 = m_ndcg5 / len(cases)\n",
    "    index.m_recall = m_recall / len(cases)\n",
    "    index.mean_precision_11point = mean_precision_11point/len(cases)\n",
    "\n",
    "    print(\"   P10    \", index.m_p10)\n",
    "    print(\"   NDCG@5 \", index.m_ndcg5)\n",
    "    print(\"   MRR    \", index.m_mrr)\n",
    "    print(\"   MAP    \", index.m_ap)\n",
    "    print(\"   Recall \", index.m_recall)\n",
    "\n",
    "    indexes_list[index_name] = index\n",
    "\n",
    "#pickle.dump(indexes_list, open( \"indexes_results.bin\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53df4f5-198f-40ab-b047-281f020a5561",
   "metadata": {},
   "source": [
    "# Results and discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously computed results\n",
    "indexes_list = pickle.load( open( \"indexes_results.bin\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f6df6-c07a-47c2-b516-2c9eca83aa6b",
   "metadata": {},
   "source": [
    "All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4702396-3b65-4014-a784-5e7335aae436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recall_11point = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "for index_name in indexes_list:\n",
    "    index = indexes_list[index_name]\n",
    "    plt.plot(recall_11point, index.mean_precision_11point, label = index_name)\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e767b419-6e5c-4720-a9da-6351682a77c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_table = pd.DataFrame(columns =['model', 'p10', 'ndcg5', 'mrr', 'map', 'recall'])\n",
    "for index_name in indexes_list:\n",
    "    index = indexes_list[index_name]\n",
    "    aa = pd.DataFrame({'model':[index_name], 'p10':[index.m_p10], 'ndcg5':[index.m_ndcg5], 'mrr': [index.m_mrr], 'map':[index.m_ap], 'recall':[index.m_recall]})\n",
    "    results_table = results_table._append(aa, ignore_index=False)\n",
    "\n",
    "results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85680ca1-961b-4831-a58d-28b39cdf7b82",
   "metadata": {},
   "source": [
    "LETOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1a9212-48e7-4746-ab6a-9cbc4c7d560c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recall_11point = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "for index_name in indexes_list:\n",
    "    if index_name == 'LETOR_model':\n",
    "        index = indexes_list[index_name]\n",
    "        plt.plot(recall_11point, index.mean_precision_11point, label = index_name)\n",
    "        plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614b3680-3876-4e24-875a-7aa2765d018f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_table = pd.DataFrame(columns =['model', 'p10', 'ndcg5', 'mrr', 'map', 'recall'])\n",
    "\n",
    "for index_name in indexes_list:\n",
    "    if index_name == 'LETOR_model':\n",
    "        index = indexes_list[index_name]\n",
    "        aa = pd.DataFrame({'model':[index_name], 'p10':[index.m_p10], 'ndcg5':[index.m_ndcg5], 'mrr': [index.m_mrr], 'map':[index.m_ap], 'recall':[index.m_recall]})\n",
    "        results_table = results_table._append(aa, ignore_index=False)\n",
    "\n",
    "results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a02b73d-7a5d-429d-819a-5db614a6768c",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1b00fd-b511-4e15-89f6-5eec21cbb107",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ORIGINAL CELL MADE BY THE TEACHER\n",
    "index = indexes_list['lmjm_full']\n",
    "#index = indexes_list['lmjm_brief_title']\n",
    "\n",
    "query_result = np.sort([index.p10_per_query[caseid] for caseid in cases])\n",
    "query_text = np.sort([cases[caseid][0:30] for caseid in cases])\n",
    "\n",
    "figure(figsize=(8, 19), dpi=80)\n",
    "\n",
    "plt.barh(query_text, query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68abaa4-4da2-45bb-9fe5-35be6a139a90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = ['p10', 'ndcg5', 'recall', 'mrr', 'ap']\n",
    "\n",
    "def get_query_result(selected_index, selected_metric):\n",
    "    query_result = None\n",
    "    \n",
    "    if selected_metric == metrics[0]:\n",
    "        query_result = np.sort([indexes_list[selected_index].p10_per_query[caseid] for caseid in cases])\n",
    "        \n",
    "    elif selected_metric == metrics[1]:\n",
    "        query_result = np.sort([indexes_list[selected_index].ndcg5_per_query[caseid] for caseid in cases])\n",
    "        \n",
    "    elif selected_metric == metrics[2]:\n",
    "        query_result = np.sort([indexes_list[selected_index].recall_per_query[caseid] for caseid in cases])\n",
    "        \n",
    "    elif selected_metric == metrics[3]:\n",
    "        query_result = np.sort([indexes_list[selected_index].mrr_per_query[caseid] for caseid in cases])\n",
    "        \n",
    "    elif selected_metric == metrics[4]:\n",
    "        query_result = np.sort([indexes_list[selected_index].ap_per_query[caseid] for caseid in cases])\n",
    "        \n",
    "    return query_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4241091d-7100-4df6-b9ce-160cb93ff99b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CELL CREATED TO VISUALIZE THE METRICS OF EACH MODEL INDIVIDUALLY WITH EASE\n",
    "\n",
    "query_texts = np.sort([cases[caseid][0:30] for caseid in cases])\n",
    "\n",
    "def plot_barh(selected_index, selected_metric):\n",
    "    plt.figure(figsize=(8, 19), dpi=80)\n",
    "    \n",
    "    query_result = get_query_result(selected_index, selected_metric)\n",
    "        \n",
    "    y = np.arange(len(query_result))\n",
    "    \n",
    "    figure(figsize=(8, 19), dpi=80)\n",
    "\n",
    "    plt.barh(query_texts, query_result)\n",
    "\n",
    "\n",
    "index_selector = Dropdown(options=list(indexes_list.keys()))\n",
    "metric_selector = Dropdown(options=metrics)\n",
    "\n",
    "interact(plot_barh, selected_index=index_selector, selected_metric = metric_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4add0e-16fe-47c5-8263-81cc3bee2e09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CELL CREATED TO VISUALIZE THE METRICS OF ALL MODELS AT ONCE (MIGHT NEED CHANGING SO MARKERS IN THE CHART DONT OVERLAP)\n",
    "import plotly.express as px\n",
    "query_texts = np.sort([cases[caseid][0:30] for caseid in cases])\n",
    "\n",
    "# Define a function to update the scatter plot\n",
    "def update_scatter(selected_metric):\n",
    "    data = []\n",
    "\n",
    "    for key in indexes_list:\n",
    "        query_result = get_query_result(key, selected_metric)\n",
    "        data.extend([(query_result[i], query_texts[i], key) for i in range(len(query_result))])\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[selected_metric, \"Cases\", \"Model\"])\n",
    "\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=selected_metric,\n",
    "        y=\"Cases\",\n",
    "        color=\"Model\",\n",
    "        title=f\"Scatter Plot of {selected_metric}\"\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=1000,\n",
    "        width=1000,  \n",
    "    )\n",
    "    \n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# Create a dropdown menu for selecting metrics\n",
    "metric_selector = Dropdown(options=metrics)\n",
    "\n",
    "# Use the interact function to update the scatter plot based on the selected metric\n",
    "interact(update_scatter, selected_metric=metric_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619d3edb",
   "metadata": {},
   "source": [
    "Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b94a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'dmis-lab/biobert-v1.1'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "config = AutoConfig.from_pretrained(model_path,  output_hidden_states=True, output_attentions=True)  \n",
    "model = AutoModel.from_pretrained(model_path, config=config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01757117",
   "metadata": {},
   "source": [
    "Layer Embeddings Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd5bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import string\n",
    "\n",
    "# Make sure nltk's stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def get_tokens_and_outputs(query):\n",
    "\n",
    "    # Tokenize the query\n",
    "    inputs = tokenizer.encode_plus(query, return_tensors='pt', add_special_tokens=True, max_length=512, truncation=True)\n",
    "    input_ids = inputs['input_ids']\n",
    "    input_id_list = input_ids[0].tolist()  # Batch index 0\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs.to(device)\n",
    "        outputs = model(**inputs)  # Ensure 'model' is a callable PyTorch model\n",
    "\n",
    "    return tokens, outputs\n",
    "\n",
    "\n",
    "def extract_top_words_from_query(query, top_k, remove_stop_words=True):\n",
    "    # Define stop words if removal is enabled\n",
    "    stop_words = set(stopwords.words('english')) if remove_stop_words else set()\n",
    "\n",
    "    tokens, outputs = get_tokens_and_outputs(query)\n",
    "\n",
    "    attention = outputs['attentions']\n",
    "\n",
    "    # Calculate mean attention accors all heads in the last layer\n",
    "    attention_scores = torch.sum(attention[-1][0], dim=(0,1))\n",
    "\n",
    "    # Remove special tokens, punctuation, and stop words and duplicate tokens\n",
    "    token_with_scores = [(idx, token, score) for idx, (token, score) in enumerate(zip(tokens, attention_scores)) \n",
    "                         if token not in ['[CLS]', '[SEP]'] and token.lower() not in stop_words and token not in string.punctuation]\n",
    "\n",
    "\n",
    "    # Sort by score and select top X tokens' positions without duplicates\n",
    "    top_token_positions = []\n",
    "    added_tokens = set()\n",
    "    for idx, _, _ in sorted(token_with_scores, key=lambda x: x[2], reverse=True):\n",
    "        token = tokens[idx]\n",
    "        if token not in added_tokens:\n",
    "            top_token_positions.append(idx)\n",
    "            added_tokens.add(token)\n",
    "            if len(top_token_positions) == top_k:\n",
    "                break\n",
    "\n",
    "    return top_token_positions\n",
    "\n",
    "\n",
    "def plot_embeddings(embeddings1, embeddings2, tokens):\n",
    "    transformed_embeddings1 = PCA().fit_transform(embeddings1)[:,:2]\n",
    "    transformed_embeddings2 = PCA().fit_transform(embeddings2)[:,:2]\n",
    "    \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    plt.scatter(transformed_embeddings1[:,0], transformed_embeddings1[:,1], edgecolors='k', c='r')\n",
    "    plt.scatter(transformed_embeddings2[:,0], transformed_embeddings2[:,1], edgecolors='k', c='b')\n",
    "\n",
    "    for token, (x1, y1), (x2, y2) in zip(tokens, transformed_embeddings1, transformed_embeddings2):\n",
    "        plt.text(x1+0.05, y1+0.05, token)\n",
    "        plt.text(x2+0.05, y2+0.05, token)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e7ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = cases[\"20141\"]\n",
    "doc = doc_dict[\"NCT00000492\"]._detailed_description\n",
    "query_doc_pair = (query, doc)\n",
    "\n",
    "query_tokens, outputs = get_tokens_and_outputs(query_doc_pair)\n",
    "top_positions = extract_top_words_from_query(query_doc_pair, top_k=10, remove_stop_words=True)\n",
    "sorted_top_positions = sorted(top_positions)\n",
    "\n",
    "top_tokens = [query_tokens[i] for i in sorted_top_positions]\n",
    "\n",
    "# Get the embeddings of the first and last layers for the top tokens\n",
    "embeddings_first_layer = outputs['hidden_states'][0][:, top_positions, :].cpu().numpy().squeeze()\n",
    "embeddings_last_layer = outputs['hidden_states'][-1][:, top_positions, :].cpu().numpy().squeeze()\n",
    "#embeddings_first_layer = outputs['hidden_states'][0][:, :, :].cpu().numpy().squeeze()\n",
    "#embeddings_last_layer = outputs['hidden_states'][-1][:, :, :].cpu().numpy().squeeze()\n",
    "\n",
    "pp.pprint(query_doc_pair)\n",
    "pp.pprint(top_tokens)\n",
    "\n",
    "plot_embeddings(embeddings_first_layer, embeddings_last_layer, top_tokens)\n",
    "#plot_embeddings(embeddings_first_layer, embeddings_last_layer, query_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647e16e3",
   "metadata": {},
   "source": [
    "Layer Embeddings Similarity Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfa55bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = outputs['attentions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1599b1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_matrix(layer, tokens, attention, specific_positions=None):\n",
    "    multiplier = len(tokens) / 3\n",
    "    rows = 3\n",
    "    cols = 4\n",
    "    fig, ax_full = plt.subplots(rows, cols)\n",
    "    fig.set_figheight(rows*multiplier)\n",
    "    fig.set_figwidth(cols*multiplier+3)\n",
    "    plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "    j = 0\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "        \n",
    "            ax = ax_full[r,c]\n",
    "            \n",
    "            # Get the attention scores for the j-th head in the i-th layer\n",
    "            sattention = attention[layer][0][j].cpu().numpy()\n",
    "\n",
    "            # Filter attention scores for specific positions if provided\n",
    "            if specific_positions is not None:\n",
    "                # Sort positions in ascending order\n",
    "                sattention = sattention[specific_positions, :][:, specific_positions]\n",
    "          \n",
    "            sattention = np.flip(sattention, 0)  \n",
    "            \n",
    "            plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "            im = ax.pcolormesh(sattention, cmap='gnuplot')\n",
    "\n",
    "            # Show all ticks and label them with the respective list entries\n",
    "            ax.set_title(\"Head \" + str(j))\n",
    "            ax.set_yticks(np.arange(len(tokens)))\n",
    "            if c == 0:\n",
    "                ax.set_yticklabels(reversed(tokens))\n",
    "                ax.set_ylabel(\"Queries\")\n",
    "            else:\n",
    "                ax.set_yticks([])\n",
    "\n",
    "            ax.set_xticks(np.arange(len(tokens)))\n",
    "            if r == rows-1:\n",
    "                ax.set_xticklabels(tokens)\n",
    "                ax.set_xlabel(\"Keys\")\n",
    "                \n",
    "                # Rotate the tick labels and set their alignment.\n",
    "                plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "                        rotation_mode=\"anchor\")\n",
    "            else:\n",
    "                ax.set_xticks([])\n",
    "\n",
    "                \n",
    "            # Loop over data dimensions and create text annotations.\n",
    "            j = j + 1\n",
    "\n",
    "    fig.suptitle(\"Layer\" + str(layer) + \" Multi-head Self-attentions\")\n",
    "    cbar = fig.colorbar(im, ax=ax_full, location='right', shrink=0.5)\n",
    "    cbar.ax.set_ylabel(\"Selt-attention\", rotation=-90, va=\"bottom\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd06542",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = -1\n",
    "plot_matrix(layer, top_tokens, attention, specific_positions=sorted_top_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70facaa7",
   "metadata": {},
   "source": [
    "Self-attention head visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db7a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_html():\n",
    "  import IPython\n",
    "  display(IPython.core.display.HTML('''\n",
    "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
    "        <script>\n",
    "          requirejs.config({\n",
    "            paths: {\n",
    "              base: '/static/base',\n",
    "              \"d3\": \"https://cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min\",\n",
    "              jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
    "            },\n",
    "          });\n",
    "        </script>\n",
    "        '''))\n",
    "\n",
    "call_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c9719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_view(attention, query_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81492ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_view(attention, query_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98275ad6",
   "metadata": {},
   "source": [
    "# Bonus Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd69a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (maybe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-cv-ir",
   "language": "python",
   "name": "nlp-cv-ir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
